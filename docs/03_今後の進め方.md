# 今後の進め方

PoC を体験した後、自分たちのデータで予知保全を実現するための具体的なアプローチを段階的にまとめる。

---

## フェーズ全体像

```
Phase 1          Phase 2          Phase 3          Phase 4
データ準備        小規模検証        本格運用          拡張・改善
(2〜4週間)       (2〜4週間)       (1〜2ヶ月)       (継続的)
───────────     ───────────     ───────────     ───────────
・データ調査      ・PoC環境で      ・本番環境構築    ・モデル再学習
・収集方法決定      自社データ投入  ・アラート運用    ・設備追加
・品質確認        ・ML検証          ・ダッシュボード  ・精度改善
                 ・有効性判断        本番化
```

---

## Phase 1: データの準備（最も重要）

予知保全の成否の8割はデータで決まる。モデルやツールの選定よりも、**どんなデータがあり、どう使えるか**を徹底的に調べることが最優先。

### 1-1. 現状のデータを棚卸しする

以下の質問に答えられるようにする:

| 質問 | 回答例 | 重要度 |
|---|---|---|
| どんな設備を監視したいか？ | ポンプ3台、モーター5台 | 必須 |
| どんなセンサーがついているか？ | 振動、温度、圧力、流量 | 必須 |
| データはどこに保存されているか？ | PLC → SCADA → CSV/DB | 必須 |
| データの取得間隔は？ | 1秒/10秒/1分/10分 | 必須 |
| データはいつから蓄積されているか？ | 過去1年分ある | 重要 |
| 故障の記録はあるか？ | 保全日誌に手書きで残っている | 重要 |
| データの欠損やノイズはあるか？ | たまに通信エラーで欠損する | 重要 |

### 1-2. 故障履歴を整理する

Python ML（教師あり学習）を使うには、故障履歴が不可欠。以下の形式で整理する:

```csv
device_id,failure_date,failure_mode,description
pump-01,2025-06-15,ベアリング劣化,振動異常により停止
pump-01,2025-09-20,シール漏れ,圧力低下で検知
motor-01,2025-08-10,巻線劣化,電流異常上昇
```

> **故障履歴がない場合**: まずは Elasticsearch ML（教師なし）から始める。並行して、今後の故障を記録する仕組みを作る。半年〜1年分の履歴が溜まったら教師あり学習に移行する。

### 1-3. データ品質を確認する

実データは PoC のサンプルデータと違い、以下の問題がある:

| 問題 | 具体例 | 対処 |
|---|---|---|
| **欠損値** | 通信エラーでデータが途切れる | 補間（前の値で埋める）or 除外 |
| **異常値** | センサー故障で 0 や 99999 が入る | 閾値で除外 |
| **時刻ずれ** | 設備ごとにタイムスタンプがずれている | NTP同期、取込時に補正 |
| **単位の不統一** | ある設備は℃、別の設備は°F | Logstash の filter で統一 |
| **重複データ** | 再送で同じデータが2回入る | Elasticsearch の `_id` で重複排除 |

---

## Phase 2: 小規模検証

### 2-1. PoC 環境に自社データを投入する

PoC の環境をそのまま使い、Logstash パイプラインを自社データに合わせて書き換える。

**変更が必要なファイル**:

1. **`logstash/pipeline/sensor-data.conf`** — 自社データの形式に合わせる

```ruby
# 例: DB からデータを取得する場合
input {
  jdbc {
    jdbc_connection_string => "jdbc:postgresql://db-host:5432/sensor_db"
    jdbc_user => "readonly_user"
    jdbc_password => "${DB_PASSWORD}"
    schedule => "*/5 * * * *"   # 5分ごとに取得
    statement => "SELECT * FROM sensor_readings WHERE timestamp > :sql_last_value"
  }
}
```

2. **`data/failure_history.csv`** — 自社の故障履歴に差し替え

3. **`python/train_model.py`** — 特徴量を自社データに合わせて調整

### 2-2. Elasticsearch ML で検証する

まずはコーディング不要の Elasticsearch ML で、自社データの異常検知を試す。

手順は PoC の Step 3 と同じだが、以下の点を自社データに合わせて変更:

| 設定 | PoC での値 | 自社データでの検討 |
|---|---|---|
| データビューのインデックスパターン | `sensor-data-*` | 自社のインデックス名に変更 |
| 分析対象のメトリック | vibration, temperature, current | 自社のセンサーフィールドに変更 |
| Influencers | device_id.keyword | 自社の設備ID フィールドに変更 |
| バケットスパン | 15m | データの取得間隔に応じて調整 |

### 2-3. 結果を評価する

以下の観点で検証結果を評価する:

- **既知の故障を検知できたか？** → 過去の故障日の前に異常スコアが上昇していれば成功
- **誤検知はどの程度か？** → 正常なのに異常と判定されたケースの数
- **検知タイミングは十分か？** → 故障の何日前に検知できたか（早ければ早いほど良い）
- **運用者にとって理解しやすいか？** → ダッシュボードを見て状況が把握できるか

### 2-4. 有効性を判断する

| 結果 | 判断 | 次のアクション |
|---|---|---|
| 既知の故障を概ね検知できた | 有効 → Phase 3 へ | 本番環境の構築を開始 |
| 検知できるが誤検知が多い | 改善の余地あり | 特徴量やパラメータを調整 |
| ほとんど検知できない | データの見直し | センサーの追加、データ品質の改善 |

---

## Phase 3: 本番環境の構築

### 3-1. 環境の選択

| 環境 | メリット | デメリット | 推奨シーン |
|---|---|---|---|
| **ローカル Docker** | コスト0、高速試行 | スケーラビリティ限界 | 小規模（設備10台以下） |
| **Azure Elastic Cloud** | マネージド、高可用性 | コスト発生 | 中〜大規模、本番運用 |
| **Azure VM + Docker** | 柔軟性が高い | 運用負荷 | 中規模、カスタマイズ重視 |

> Azure 構成との対応は [00_概要と目的.md](00_概要と目的.md) の「Azure構成との対応関係」を参照。

### 3-2. データパイプラインの本番化

PoC（CSV バッチ取込）から本番（リアルタイム取込）への移行で考慮すべきこと:

```
PoC:    CSV → Logstash → Elasticsearch（バッチ、1回だけ）

本番:   PLC/SCADA → Logstash/Beats → Elasticsearch（リアルタイム、継続的）
```

| 検討項目 | 内容 |
|---|---|
| データ取得方式 | Logstash（pull型）or Filebeat/Metricbeat（push型） |
| 取得間隔 | リアルタイム or バッチ（5分/1時間等） |
| エラー処理 | 接続切断時のリトライ、Dead Letter Queue |
| バックプレッシャー | Elasticsearch の処理が追いつかない場合の対処 |

### 3-3. アラート運用の設計

| 検討項目 | 決めるべきこと |
|---|---|
| 誰に通知するか | 保全担当者？管理者？シフトによって変える？ |
| 何を通知するか | 設備名、異常内容、推奨アクション |
| どう通知するか | メール、Slack、Teams、ダッシュボード表示 |
| いつ通知するか | 即時？まとめて？営業時間内のみ？ |
| 通知後のフロー | アラートを受けた後の対応手順（ワークフロー） |

### 3-4. ダッシュボードの本番設計

PoC のダッシュボードをベースに、運用者の視点で再設計する:

**必要な画面**:

1. **全設備サマリー** — 全設備の状態を一目で把握（正常/注意/異常の台数）
2. **個別設備詳細** — 特定設備のセンサー推移と異常スコア
3. **アラート履歴** — 過去のアラート一覧と対応状況

**運用者へのヒアリング項目**:
- 今の監視方法は？（巡回？画面？）
- どの情報が最も重要？
- 判断に必要な時間軸は？（直近1時間？1日？1週間？）

---

## Phase 4: 拡張と改善（継続的）

### 4-1. モデルの再学習

MLモデルは時間とともに精度が低下する（データドリフト）。定期的な再学習が必要。

```
初回学習 → 運用開始 → 新しい故障データが蓄積 → モデル再学習 → 精度向上
                                    ↑                          │
                                    └──────────────────────────┘
                                          改善サイクル
```

再学習のタイミング:
- 新しい故障が発生した後
- 設備の改修・部品交換後
- 季節変動がある場合は四半期ごと

### 4-2. 対象設備の拡大

1台で成功したら、同種の設備に横展開する:

```
Step 1: パイロット設備（1台）で成功を確認
Step 2: 同種の設備（同型ポンプ5台等）に展開
Step 3: 異種設備（モーター、コンプレッサー等）に拡大
```

異種設備に展開する場合、センサーの種類や正常値が異なるため、設備タイプごとにモデルを作り直す必要がある。

### 4-3. 高度な分析への発展

| レベル | 内容 | 今回の PoC での対応 |
|---|---|---|
| レベル1: 異常検知 | 「何かおかしい」を検知 | Elasticsearch ML |
| レベル2: 故障予測 | 「いつ壊れるか」を予測 | Python ML（予兆スコア） |
| レベル3: 原因診断 | 「なぜおかしいか」を特定 | 特徴量重要度で部分的に対応 |
| レベル4: 処方提案 | 「何をすべきか」を提案 | 未対応（今後の課題） |

---

## よくある失敗パターンと対策

### 失敗1: データなしでツール選定から始める

```
❌ 「ELK を導入しよう」→ 環境構築 → データがない → 頓挫
✅ 「データを調べよう」→ 何が使えるか把握 → 適切なツールを選定
```

### 失敗2: いきなり全設備を対象にする

```
❌ 工場の全設備100台を一度に対象 → 複雑すぎて進まない
✅ 重要な設備1台で成功 → 横展開
```

### 失敗3: 精度100%を目指す

```
❌ 「誤検知ゼロ」を目指して永遠にチューニング
✅ 「検知漏れよりは誤検知のほうがマシ」と割り切って運用開始
```

### 失敗4: 現場を巻き込まない

```
❌ IT部門だけで作って現場に渡す → 使われない
✅ 保全担当者と一緒にダッシュボードを設計 → 実際の業務に組み込む
```

---

## チェックリスト: 次のステップに進む前に

### Phase 1 に進む前に
- [ ] 監視対象の設備を決めた（まずは1〜2台）
- [ ] その設備にどんなセンサーがあるか把握した
- [ ] データの保存場所とアクセス方法を確認した
- [ ] 過去の故障履歴を整理した（なければ「なし」で進む方針を決めた）

### Phase 2 に進む前に
- [ ] 自社データを CSV または DB から取得できた
- [ ] Logstash パイプラインを自社データ形式に書き換えた
- [ ] PoC 環境にデータを投入し、Kibana で閲覧できた

### Phase 3 に進む前に
- [ ] Elasticsearch ML または Python ML で既知の異常を検知できた
- [ ] 運用者がダッシュボードを見て「使えそう」と評価した
- [ ] 本番環境（ローカル/クラウド）の方針を決めた
- [ ] アラートの通知先と対応フローを設計した

---

## 参考: PoC の環境を再利用する

今回の PoC 環境は、Phase 2 の小規模検証にそのまま使える。

```bash
# PoC 環境のクリーンアップ（サンプルデータを削除）
docker compose down -v

# 自社データ用に再起動
docker compose up -d

# Logstash パイプラインを書き換えて、自社データを投入
# → 以降は PoC と同じ手順で Data View 作成、ML ジョブ作成、ダッシュボード作成
```

PoC で学んだ手順（[01_学習手順.md](01_学習手順.md)）と概念（[02_より理解をすすめるために.md](02_より理解をすすめるために.md)）を組み合わせて、自分たちのデータで実践する。
